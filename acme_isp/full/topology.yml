---
# netlab default info for provider and device type
defaults:
  device: frr
  provider: clab
  # when running on github codespaces we cannot use VRFs...
  devices.frr.clab.group_vars.netlab_mgmt_vrf: false
plugin: [ bgp.session, bgp.policy ]


# address pools
## let's define here the main address pools that will be used on the links
## some pools will contain only one subnet, some others will contain multiple subnets
##  that will be dynamically allocated to the different links using it.
addressing:
  # Transit 1 & 2 P2P Links (let's assume 100.71.x.x for T1 and 100.72.x.x for T2)
  t1_p2p:
    ipv4: 100.71.0.0/24
    prefix: 30
  t2_p2p:
    ipv4: 100.72.0.0/24
    prefix: 30
  # Internet Exchange LAN
  ixp_lan:
    ipv4: 100.70.70.0/24
    prefix: 24
  # WINstagram pool (let's assume WINstagram will have reserved 40.40.40.0/24)
  wins_lan:
    ipv4: 100.80.44.0/24
    prefix: 24
  # Newspaper-X pool (let's assume Newspaper-X will have reserved 50.50.50.0/24)
  nx_lan:
    ipv4: 100.90.55.0/24
    prefix: 24
  # ACME pools (from 1.100.0.0/20 - 1.100.0.0-1.100.15.255)
  # loopbacks (need to override main loopback pool) - let's use the first /24 for loopbacks
  loopback:
    ipv4: 100.100.0.0/24
    prefix: 32
  ## ACME p2p (internal)
  acme_p2p:
    ipv4: 100.100.1.0/24
    prefix: 30
  ## ACME p2p (towards business customers)
  acme_p2p_cust:
    ipv4: 100.100.2.0/24
    prefix: 30
  ## BRAS Dynamic Pool for residential customers
  bras_lan:
    ipv4: 100.100.3.0/24
    prefix: 24
  ## Customer 1 Delegated Pool
  cust1_lan:
    ipv4: 100.100.15.0/24
    prefix: 24


# definition of nodes/virtual devices and specific attributes
## `id` attribute is not mandatory, but we can set it to control which loopback address will be used by every router
## NOTE: every node (apart from linux hosts) will be assigned a loopback address by default.
nodes:
# ACME ISP
  core1:
    id: 1
  core2:
    id: 2
  igw1:
    id: 11
  igw2:
    id: 12
  ixp:
    id: 13
  corp1:
    id: 14
  corp2:
    id: 15
  bras:
    id: 16
# ACME Customers
## Customer 1, Routers + Host
  c1rt:
    id: 20
  c1host:
    id: 21
    device: linux
    image: ssasso/netlab-linux-host:latest
## Customer 2 and 3, only Hosts
  c2host:
    id: 22
    device: linux
    image: ssasso/netlab-linux-host:latest
  c3host:
    id: 23
    device: linux
    image: ssasso/netlab-linux-host:latest
# Transit Providers
  transit1:
    id: 31
    loopback:
      ipv4: 100.71.1.1/32
  transit2:
    id: 32
    loopback:
      ipv4: 100.72.2.2/32
# WINstagram
  winsrt:
    id: 40
    loopback:
      ipv4: 100.80.80.80/32
  winsweb:
    id: 44
    device: linux
    image: ssasso/pyweb:latest
    clab.env:
      WEB_MESSAGE: 'Hey, this is WINstagram!'
# Newspaper-X
  nxrt:
    id: 50
    loopback:
      ipv4: 100.90.90.90/32
  nxweb:
    id: 55
    device: linux
    image: ssasso/pyweb:latest
    clab.env:
      WEB_MESSAGE: 'Welcome to Newspaper-X.'


#### GROUP SETTINGS BELOW. Used to define modules and testing configuration.
#### Comment this for normal workshop usage.
###### BGP Policy tweaks:
###### - we need to create, on border routers, fake med/local_preferences, to have a route-map attached to the neighbor config.
groups:
  acme_core:
    members:
    - core1
    - core2
    module: [ ospf, bgp ]
    config: [ extra/bgp-add-path.j2 ]
    bgp:
      as: 64666
      advertise_loopback: true
      rr: true
      # generate the aggregate route
      originate: [ 100.100.0.0/20 ]
  acme_rt:
    members:
    - igw1
    - igw2
    - ixp
    - corp1
    - corp2
    - bras
    module: [ ospf, bgp ]
    config: [ extra/bgp-add-path.j2, extra ]
    bgp:
      as: 64666
      advertise_loopback: true
  c1_routers:
    members: [ c1rt ]
    module: [ bgp ]
    config: [ extra ]
    bgp:
      as: 65535
  t1_routers:
    members: [ transit1 ]
    module: [ bgp ]
    bgp:
      as: 65001
      originate: [ 100.71.0.0/16 ]
  t2_routers:
    members: [ transit2 ]
    module: [ bgp ]
    bgp:
      as: 65002
      originate: [ 100.72.0.0/16 ]
  w_routers:
    members: [ winsrt ]
    module: [ bgp ]
    config: [ extra ]
    bgp:
      as: 65040
      advertise_loopback: false
  n_routers:
    members: [ nxrt ]
    module: [ bgp ]
    config: [ extra ]
    bgp:
      as: 65050
      advertise_loopback: false


# definition of all the links between the different virtual devices
links:
# ACME Internal Links

## core1 to core2
- core1:
  core2:
  pool: acme_p2p

## IGW 1 to C1 and C2
- core1:
  igw1:
  pool: acme_p2p
- core2:
  igw1:
  pool: acme_p2p

## IGW 2 to C1 and C2
- core1:
  igw2:
  pool: acme_p2p
- core2:
  igw2:
  pool: acme_p2p

## IXP to C1 and C2
- core1:
  ixp:
  pool: acme_p2p
- core2:
  ixp:
  pool: acme_p2p

## CORP1 to C1 and C2
- core1:
  corp1:
  pool: acme_p2p
- core2:
  corp1:
  pool: acme_p2p

## CORP2 to C1 and C2
- core1:
  corp2:
  pool: acme_p2p
- core2:
  corp2:
  pool: acme_p2p

## BRAS to C1 and C2
- core1:
  bras:
  pool: acme_p2p
- core2:
  bras:
  pool: acme_p2p

# ACME Transit and Peerings

## transit 1 - policy: this needs to be preferred over transit 2
- transit1:
  igw1:
    bgp:
      remove_private_as: true
      med: 100
      locpref: 100
  pool: t1_p2p

## transit 2 - policy: backup transit
- transit2:
  igw2:
    bgp:
      remove_private_as: true
      med: 500
      locpref: 50
  pool: t2_p2p

## ixp - policy: this needs to be preferred over all transits.
- ixp:
    bgp:
      remove_private_as: true
      med: 20
      locpref: 200
  winsrt:
    bgp:
      med: 20
      locpref: 200
  pool: ixp_lan

# ACME Customers

## Cust-1
- corp1:
    bgp:
      advertise: true
      locpref: 10
  c1rt:
    bgp:
      med: 20
      locpref: 200
  pool: acme_p2p_cust
- corp2:
    bgp:
      advertise: true
      locpref: 10
  c1rt:
    bgp:
      med: 100
      locpref: 100
  pool: acme_p2p_cust
- c1rt:
    bgp.advertise: true
  c1host:
  pool: cust1_lan

## BRAS "dynamic" LAN
- bras:
    bgp.advertise: true
  c2host:
  c3host:
  pool: bras_lan

# WINstagram
- transit1:
  winsrt:
    bgp:
      med: 100
      locpref: 100
  pool: t1_p2p
- transit2:
  winsrt:
    bgp:
      med: 500
      locpref: 50
  pool: t2_p2p
- winsrt:
    bgp.advertise: true
  winsweb:
  pool: wins_lan

# Newspaper-X
- transit1:
  nxrt:
    bgp:
      med: 100
      locpref: 100
  pool: t1_p2p
- transit2:
  nxrt:
    bgp:
      med: 500
      locpref: 50
  pool: t2_p2p
- nxrt:
    bgp.advertise: true
  nxweb:
  pool: nx_lan

# let's assume transit 1 and transit 2 are linked each other, like it happens for Tier 1 operators
- transit1:
  transit2:
  pool: t1_p2p


# validation
_include: [ ../validate.yml ]
